
==> Audit <==
|------------|--------------------------------|----------|-------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |    User     | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|-------------|---------|---------------------|---------------------|
| service    | web-interface                  | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:27 PDT |                     |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:30 PDT | 06 Jun 24 12:31 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:31 PDT | 06 Jun 24 12:31 PDT |
|            | powershell                     |          |             |         |                     |                     |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:31 PDT | 06 Jun 24 12:31 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:34 PDT | 06 Jun 24 12:34 PDT |
|            | powershell                     |          |             |         |                     |                     |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:36 PDT | 06 Jun 24 12:36 PDT |
|            | powershell                     |          |             |         |                     |                     |
| stop       |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:38 PDT | 06 Jun 24 12:38 PDT |
| delete     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:38 PDT | 06 Jun 24 12:38 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:40 PDT | 06 Jun 24 12:40 PDT |
|            | powershell -u                  |          |             |         |                     |                     |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:40 PDT | 06 Jun 24 12:41 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:41 PDT | 06 Jun 24 12:41 PDT |
|            | powershell                     |          |             |         |                     |                     |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:42 PDT | 06 Jun 24 12:42 PDT |
|            | powershell                     |          |             |         |                     |                     |
| tunnel     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 12:44 PDT | 06 Jun 24 12:45 PDT |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:04 PDT | 06 Jun 24 13:04 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:04 PDT | 06 Jun 24 13:04 PDT |
|            | powershell                     |          |             |         |                     |                     |
| tunnel     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:07 PDT |                     |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:08 PDT | 06 Jun 24 13:08 PDT |
|            | powershell                     |          |             |         |                     |                     |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:16 PDT | 06 Jun 24 13:16 PDT |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:16 PDT | 06 Jun 24 13:17 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:17 PDT | 06 Jun 24 13:17 PDT |
|            | powershell                     |          |             |         |                     |                     |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:17 PDT | 06 Jun 24 13:17 PDT |
| stop       |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:18 PDT | 06 Jun 24 13:18 PDT |
| delete     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:18 PDT |                     |
| stop       |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:18 PDT |                     |
| delete     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:18 PDT | 06 Jun 24 13:18 PDT |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:18 PDT | 06 Jun 24 13:19 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:19 PDT | 06 Jun 24 13:19 PDT |
|            | powershell                     |          |             |         |                     |                     |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:20 PDT | 06 Jun 24 13:20 PDT |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:20 PDT | 06 Jun 24 13:20 PDT |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:33 PDT | 06 Jun 24 13:33 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:33 PDT | 06 Jun 24 13:33 PDT |
|            | powershell                     |          |             |         |                     |                     |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:36 PDT | 06 Jun 24 13:36 PDT |
|            | powershell                     |          |             |         |                     |                     |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:37 PDT | 06 Jun 24 13:37 PDT |
| tunnel     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:40 PDT |                     |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:49 PDT | 06 Jun 24 13:49 PDT |
| stop       |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:49 PDT | 06 Jun 24 13:49 PDT |
| delete     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:50 PDT |                     |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:50 PDT | 06 Jun 24 13:50 PDT |
| stop       |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:50 PDT | 06 Jun 24 13:50 PDT |
| delete     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:50 PDT | 06 Jun 24 13:50 PDT |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 13:50 PDT | 06 Jun 24 13:50 PDT |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:28 PDT | 06 Jun 24 14:28 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:28 PDT | 06 Jun 24 14:28 PDT |
|            | powershell                     |          |             |         |                     |                     |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:29 PDT | 06 Jun 24 14:29 PDT |
| stop       |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:39 PDT | 06 Jun 24 14:39 PDT |
| delete     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:39 PDT | 06 Jun 24 14:40 PDT |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:40 PDT | 06 Jun 24 14:40 PDT |
| tunnel     |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:42 PDT |                     |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:42 PDT | 06 Jun 24 14:42 PDT |
| start      | --driver=docker                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:44 PDT | 06 Jun 24 14:44 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:45 PDT | 06 Jun 24 14:45 PDT |
|            | powershell                     |          |             |         |                     |                     |
| docker-env | minikube docker-env            | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:45 PDT | 06 Jun 24 14:45 PDT |
| ip         |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 14:50 PDT | 06 Jun 24 14:50 PDT |
| start      |                                | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 15:02 PDT | 06 Jun 24 15:02 PDT |
| docker-env | minikube docker-env            | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 15:02 PDT | 06 Jun 24 15:02 PDT |
| docker-env | minikube docker-env            | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 15:02 PDT | 06 Jun 24 15:02 PDT |
| docker-env | minikube docker-env --shell    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 15:03 PDT | 06 Jun 24 15:03 PDT |
|            | powershell                     |          |             |         |                     |                     |
| docker-env | minikube docker-env            | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 15:10 PDT | 06 Jun 24 15:10 PDT |
| service    | web-service                    | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 15:11 PDT |                     |
| service    | web-service-service            | minikube | EPONA\caleb | v1.33.1 | 06 Jun 24 15:12 PDT |                     |
|------------|--------------------------------|----------|-------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/06/06 15:02:06
Running on machine: Epona
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0606 15:02:06.780688   17772 out.go:291] Setting OutFile to fd 104 ...
I0606 15:02:06.781689   17772 out.go:343] isatty.IsTerminal(104) = true
I0606 15:02:06.781689   17772 out.go:304] Setting ErrFile to fd 108...
I0606 15:02:06.781689   17772 out.go:343] isatty.IsTerminal(108) = true
I0606 15:02:06.781689   17772 oci.go:576] shell is pointing to dockerd inside minikube. will unset to use host
I0606 15:02:06.796486   17772 out.go:298] Setting JSON to false
I0606 15:02:06.799695   17772 start.go:129] hostinfo: {"hostname":"Epona","uptime":8098,"bootTime":1717703228,"procs":280,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3593 Build 22631.3593","kernelVersion":"10.0.22631.3593 Build 22631.3593","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"bab5fc46-56c4-434b-9568-6c01c3799c90"}
W0606 15:02:06.800214   17772 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0606 15:02:06.800731   17772 out.go:177] üòÑ  minikube v1.33.1 on Microsoft Windows 11 Home 10.0.22631.3593 Build 22631.3593
I0606 15:02:06.801769   17772 notify.go:220] Checking for updates...
I0606 15:02:06.801769   17772 out.go:177]     ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
I0606 15:02:06.802806   17772 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0606 15:02:06.802806   17772 driver.go:392] Setting default libvirt URI to qemu:///system
I0606 15:02:06.897844   17772 docker.go:122] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I0606 15:02:06.904559   17772 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0606 15:02:07.082851   17772 info.go:266] docker info: {ID:8847e6c4-b7b5-4210-b893-ffe5ea14233f Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:69 OomKillDisable:true NGoroutines:87 SystemTime:2024-06-06 22:02:07.051607886 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8229654528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0606 15:02:07.083366   17772 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0606 15:02:07.083882   17772 start.go:297] selected driver: docker
I0606 15:02:07.083882   17772 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\caleb:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0606 15:02:07.083882   17772 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0606 15:02:07.097744   17772 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0606 15:02:07.266094   17772 info.go:266] docker info: {ID:8847e6c4-b7b5-4210-b893-ffe5ea14233f Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:69 OomKillDisable:true NGoroutines:87 SystemTime:2024-06-06 22:02:07.235800725 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8229654528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0606 15:02:07.314844   17772 cni.go:84] Creating CNI manager for ""
I0606 15:02:07.314844   17772 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0606 15:02:07.314844   17772 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\caleb:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0606 15:02:07.325399   17772 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0606 15:02:07.325970   17772 cache.go:121] Beginning downloading kic base image for docker with docker
I0606 15:02:07.326547   17772 out.go:177] üöú  Pulling base image v0.0.44 ...
I0606 15:02:07.327051   17772 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0606 15:02:07.327051   17772 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0606 15:02:07.327572   17772 preload.go:147] Found local preload: C:\Users\caleb\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0606 15:02:07.327572   17772 cache.go:56] Caching tarball of preloaded images
I0606 15:02:07.327572   17772 preload.go:173] Found C:\Users\caleb\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0606 15:02:07.327572   17772 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0606 15:02:07.327572   17772 profile.go:143] Saving config to C:\Users\caleb\.minikube\profiles\minikube\config.json ...
I0606 15:02:07.420890   17772 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0606 15:02:07.420890   17772 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0606 15:02:07.420890   17772 cache.go:194] Successfully downloaded all kic artifacts
I0606 15:02:07.420890   17772 start.go:360] acquireMachinesLock for minikube: {Name:mk13546bea679848428e0e4318664a422545290b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0606 15:02:07.420890   17772 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0606 15:02:07.421408   17772 start.go:96] Skipping create...Using existing machine configuration
I0606 15:02:07.421408   17772 fix.go:54] fixHost starting: 
I0606 15:02:07.421925   17772 out.go:177] üìå  Noticed you have an activated docker-env on docker driver in this terminal:
W0606 15:02:07.422441   17772 out.go:239] ‚ùó  Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I0606 15:02:07.435994   17772 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0606 15:02:07.513688   17772 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0606 15:02:07.513688   17772 fix.go:138] unexpected machine state, will restart: <nil>
I0606 15:02:07.514192   17772 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0606 15:02:07.514709   17772 machine.go:94] provisionDockerMachine start ...
I0606 15:02:07.520908   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:07.609270   17772 main.go:141] libmachine: Using SSH client type: native
I0606 15:02:07.609270   17772 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xada3c0] 0xadcfa0 <nil>  [] 0s} 127.0.0.1 53272 <nil> <nil>}
I0606 15:02:07.609270   17772 main.go:141] libmachine: About to run SSH command:
hostname
I0606 15:02:07.728337   17772 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0606 15:02:07.728337   17772 ubuntu.go:169] provisioning hostname "minikube"
I0606 15:02:07.735063   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:07.825313   17772 main.go:141] libmachine: Using SSH client type: native
I0606 15:02:07.825313   17772 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xada3c0] 0xadcfa0 <nil>  [] 0s} 127.0.0.1 53272 <nil> <nil>}
I0606 15:02:07.825313   17772 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0606 15:02:07.955145   17772 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0606 15:02:07.961889   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:08.057940   17772 main.go:141] libmachine: Using SSH client type: native
I0606 15:02:08.057940   17772 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xada3c0] 0xadcfa0 <nil>  [] 0s} 127.0.0.1 53272 <nil> <nil>}
I0606 15:02:08.057940   17772 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0606 15:02:08.178645   17772 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0606 15:02:08.178645   17772 ubuntu.go:175] set auth options {CertDir:C:\Users\caleb\.minikube CaCertPath:C:\Users\caleb\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\caleb\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\caleb\.minikube\machines\server.pem ServerKeyPath:C:\Users\caleb\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\caleb\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\caleb\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\caleb\.minikube}
I0606 15:02:08.178645   17772 ubuntu.go:177] setting up certificates
I0606 15:02:08.178645   17772 provision.go:84] configureAuth start
I0606 15:02:08.185883   17772 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0606 15:02:08.269287   17772 provision.go:143] copyHostCerts
I0606 15:02:08.269287   17772 exec_runner.go:144] found C:\Users\caleb\.minikube/key.pem, removing ...
I0606 15:02:08.269287   17772 exec_runner.go:203] rm: C:\Users\caleb\.minikube\key.pem
I0606 15:02:08.269809   17772 exec_runner.go:151] cp: C:\Users\caleb\.minikube\certs\key.pem --> C:\Users\caleb\.minikube/key.pem (1679 bytes)
I0606 15:02:08.270329   17772 exec_runner.go:144] found C:\Users\caleb\.minikube/ca.pem, removing ...
I0606 15:02:08.270329   17772 exec_runner.go:203] rm: C:\Users\caleb\.minikube\ca.pem
I0606 15:02:08.270850   17772 exec_runner.go:151] cp: C:\Users\caleb\.minikube\certs\ca.pem --> C:\Users\caleb\.minikube/ca.pem (1074 bytes)
I0606 15:02:08.271374   17772 exec_runner.go:144] found C:\Users\caleb\.minikube/cert.pem, removing ...
I0606 15:02:08.271374   17772 exec_runner.go:203] rm: C:\Users\caleb\.minikube\cert.pem
I0606 15:02:08.271374   17772 exec_runner.go:151] cp: C:\Users\caleb\.minikube\certs\cert.pem --> C:\Users\caleb\.minikube/cert.pem (1119 bytes)
I0606 15:02:08.271895   17772 provision.go:117] generating server cert: C:\Users\caleb\.minikube\machines\server.pem ca-key=C:\Users\caleb\.minikube\certs\ca.pem private-key=C:\Users\caleb\.minikube\certs\ca-key.pem org=caleb.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0606 15:02:08.371553   17772 provision.go:177] copyRemoteCerts
I0606 15:02:08.385692   17772 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0606 15:02:08.392131   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:08.469168   17772 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53272 SSHKeyPath:C:\Users\caleb\.minikube\machines\minikube\id_rsa Username:docker}
I0606 15:02:08.561174   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0606 15:02:08.575277   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0606 15:02:08.589411   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0606 15:02:08.603866   17772 provision.go:87] duration metric: took 425.221ms to configureAuth
I0606 15:02:08.603866   17772 ubuntu.go:193] setting minikube options for container-runtime
I0606 15:02:08.603866   17772 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0606 15:02:08.610649   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:08.703715   17772 main.go:141] libmachine: Using SSH client type: native
I0606 15:02:08.703715   17772 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xada3c0] 0xadcfa0 <nil>  [] 0s} 127.0.0.1 53272 <nil> <nil>}
I0606 15:02:08.703715   17772 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0606 15:02:08.828972   17772 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0606 15:02:08.828972   17772 ubuntu.go:71] root file system type: overlay
I0606 15:02:08.828972   17772 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0606 15:02:08.835690   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:08.929567   17772 main.go:141] libmachine: Using SSH client type: native
I0606 15:02:08.930085   17772 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xada3c0] 0xadcfa0 <nil>  [] 0s} 127.0.0.1 53272 <nil> <nil>}
I0606 15:02:08.930085   17772 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0606 15:02:09.055285   17772 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0606 15:02:09.062003   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:09.150415   17772 main.go:141] libmachine: Using SSH client type: native
I0606 15:02:09.150415   17772 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xada3c0] 0xadcfa0 <nil>  [] 0s} 127.0.0.1 53272 <nil> <nil>}
I0606 15:02:09.150415   17772 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0606 15:02:09.271583   17772 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0606 15:02:09.271583   17772 machine.go:97] duration metric: took 1.7568736s to provisionDockerMachine
I0606 15:02:09.271583   17772 start.go:293] postStartSetup for "minikube" (driver="docker")
I0606 15:02:09.271583   17772 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0606 15:02:09.286619   17772 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0606 15:02:09.293305   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:09.378217   17772 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53272 SSHKeyPath:C:\Users\caleb\.minikube\machines\minikube\id_rsa Username:docker}
I0606 15:02:09.486148   17772 ssh_runner.go:195] Run: cat /etc/os-release
I0606 15:02:09.489229   17772 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0606 15:02:09.489229   17772 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0606 15:02:09.489229   17772 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0606 15:02:09.489229   17772 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0606 15:02:09.489229   17772 filesync.go:126] Scanning C:\Users\caleb\.minikube\addons for local assets ...
I0606 15:02:09.489229   17772 filesync.go:126] Scanning C:\Users\caleb\.minikube\files for local assets ...
I0606 15:02:09.489752   17772 start.go:296] duration metric: took 218.1692ms for postStartSetup
I0606 15:02:09.503973   17772 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0606 15:02:09.510651   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:09.592567   17772 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53272 SSHKeyPath:C:\Users\caleb\.minikube\machines\minikube\id_rsa Username:docker}
I0606 15:02:09.702380   17772 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0606 15:02:09.706281   17772 fix.go:56] duration metric: took 2.2848735s for fixHost
I0606 15:02:09.706281   17772 start.go:83] releasing machines lock for "minikube", held for 2.2853911s
I0606 15:02:09.712967   17772 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0606 15:02:09.797189   17772 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0606 15:02:09.805181   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:09.809927   17772 ssh_runner.go:195] Run: cat /version.json
I0606 15:02:09.816652   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:09.885747   17772 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53272 SSHKeyPath:C:\Users\caleb\.minikube\machines\minikube\id_rsa Username:docker}
I0606 15:02:09.902805   17772 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53272 SSHKeyPath:C:\Users\caleb\.minikube\machines\minikube\id_rsa Username:docker}
I0606 15:02:10.126589   17772 ssh_runner.go:195] Run: systemctl --version
I0606 15:02:10.144651   17772 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0606 15:02:10.162212   17772 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0606 15:02:10.168545   17772 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0606 15:02:10.182513   17772 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0606 15:02:10.188549   17772 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0606 15:02:10.188549   17772 start.go:494] detecting cgroup driver to use...
I0606 15:02:10.188549   17772 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0606 15:02:10.188549   17772 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0606 15:02:10.213008   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0606 15:02:10.234294   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0606 15:02:10.240956   17772 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0606 15:02:10.255632   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0606 15:02:10.277055   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0606 15:02:10.297669   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0606 15:02:10.318746   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0606 15:02:10.340420   17772 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0606 15:02:10.362173   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0606 15:02:10.383185   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0606 15:02:10.405220   17772 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0606 15:02:10.426355   17772 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0606 15:02:10.446588   17772 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0606 15:02:10.466785   17772 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 15:02:10.574527   17772 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0606 15:02:21.003953   17772 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.4294258s)
I0606 15:02:21.003953   17772 start.go:494] detecting cgroup driver to use...
I0606 15:02:21.003953   17772 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0606 15:02:21.018871   17772 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0606 15:02:21.026824   17772 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0606 15:02:21.041902   17772 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0606 15:02:21.049551   17772 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0606 15:02:21.073998   17772 ssh_runner.go:195] Run: which cri-dockerd
I0606 15:02:21.092119   17772 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0606 15:02:21.098688   17772 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0606 15:02:21.124235   17772 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0606 15:02:21.252605   17772 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0606 15:02:21.346956   17772 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0606 15:02:21.346956   17772 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0606 15:02:21.377357   17772 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 15:02:21.587815   17772 ssh_runner.go:195] Run: sudo systemctl restart docker
I0606 15:02:23.188324   17772 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.6005092s)
I0606 15:02:23.202941   17772 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0606 15:02:23.224936   17772 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0606 15:02:23.251396   17772 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0606 15:02:23.272751   17772 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0606 15:02:23.381998   17772 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0606 15:02:23.490691   17772 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 15:02:23.579867   17772 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0606 15:02:23.603608   17772 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0606 15:02:23.625686   17772 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 15:02:23.730471   17772 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0606 15:02:23.780739   17772 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0606 15:02:23.794708   17772 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0606 15:02:23.797629   17772 start.go:562] Will wait 60s for crictl version
I0606 15:02:23.812334   17772 ssh_runner.go:195] Run: which crictl
I0606 15:02:23.829299   17772 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0606 15:02:23.877347   17772 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0606 15:02:23.886369   17772 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0606 15:02:23.924848   17772 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0606 15:02:23.946909   17772 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0606 15:02:23.954731   17772 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0606 15:02:24.072070   17772 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0606 15:02:24.086661   17772 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0606 15:02:24.096370   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0606 15:02:24.180821   17772 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\caleb:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0606 15:02:24.180821   17772 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0606 15:02:24.187536   17772 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0606 15:02:24.200048   17772 docker.go:685] Got preloaded images: -- stdout --
web_interface:latest
hello_receiver:latest
hello_broadcaster:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
rabbitmq:3-management
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0606 15:02:24.200048   17772 docker.go:615] Images already preloaded, skipping extraction
I0606 15:02:24.206243   17772 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0606 15:02:24.219250   17772 docker.go:685] Got preloaded images: -- stdout --
web_interface:latest
hello_receiver:latest
hello_broadcaster:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
rabbitmq:3-management
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0606 15:02:24.219769   17772 cache_images.go:84] Images are preloaded, skipping loading
I0606 15:02:24.219769   17772 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0606 15:02:24.219769   17772 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0606 15:02:24.225975   17772 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0606 15:02:24.279135   17772 cni.go:84] Creating CNI manager for ""
I0606 15:02:24.279135   17772 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0606 15:02:24.279135   17772 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0606 15:02:24.279135   17772 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0606 15:02:24.279135   17772 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0606 15:02:24.293606   17772 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0606 15:02:24.299857   17772 binaries.go:44] Found k8s binaries, skipping transfer
I0606 15:02:24.314920   17772 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0606 15:02:24.320759   17772 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0606 15:02:24.331710   17772 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0606 15:02:24.342049   17772 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0606 15:02:24.367477   17772 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0606 15:02:24.385073   17772 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 15:02:24.506687   17772 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0606 15:02:24.514625   17772 certs.go:68] Setting up C:\Users\caleb\.minikube\profiles\minikube for IP: 192.168.49.2
I0606 15:02:24.514625   17772 certs.go:194] generating shared ca certs ...
I0606 15:02:24.514625   17772 certs.go:226] acquiring lock for ca certs: {Name:mk6dbd32fd0ee6c1b359b7fc8563f15e6901e59b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0606 15:02:24.514625   17772 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\caleb\.minikube\ca.key
I0606 15:02:24.515145   17772 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\caleb\.minikube\proxy-client-ca.key
I0606 15:02:24.515145   17772 certs.go:256] generating profile certs ...
I0606 15:02:24.515145   17772 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\caleb\.minikube\profiles\minikube\client.key
I0606 15:02:24.515665   17772 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\caleb\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0606 15:02:24.515665   17772 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\caleb\.minikube\profiles\minikube\proxy-client.key
I0606 15:02:24.516189   17772 certs.go:484] found cert: C:\Users\caleb\.minikube\certs\ca-key.pem (1679 bytes)
I0606 15:02:24.516189   17772 certs.go:484] found cert: C:\Users\caleb\.minikube\certs\ca.pem (1074 bytes)
I0606 15:02:24.516189   17772 certs.go:484] found cert: C:\Users\caleb\.minikube\certs\cert.pem (1119 bytes)
I0606 15:02:24.516710   17772 certs.go:484] found cert: C:\Users\caleb\.minikube\certs\key.pem (1679 bytes)
I0606 15:02:24.517245   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0606 15:02:24.531782   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0606 15:02:24.545660   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0606 15:02:24.560793   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0606 15:02:24.636337   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0606 15:02:24.653509   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0606 15:02:24.739339   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0606 15:02:24.758429   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0606 15:02:24.846835   17772 ssh_runner.go:362] scp C:\Users\caleb\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0606 15:02:24.942371   17772 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0606 15:02:25.047348   17772 ssh_runner.go:195] Run: openssl version
I0606 15:02:25.068480   17772 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0606 15:02:25.162343   17772 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0606 15:02:25.239879   17772 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun  6 18:36 /usr/share/ca-certificates/minikubeCA.pem
I0606 15:02:25.241984   17772 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0606 15:02:25.264124   17772 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0606 15:02:25.361752   17772 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0606 15:02:25.441141   17772 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0606 15:02:25.451185   17772 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0606 15:02:25.460070   17772 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0606 15:02:25.543437   17772 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0606 15:02:25.552993   17772 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0606 15:02:25.644357   17772 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0606 15:02:25.652900   17772 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\caleb:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0606 15:02:25.659779   17772 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0606 15:02:25.859544   17772 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0606 15:02:25.944661   17772 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0606 15:02:25.944661   17772 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0606 15:02:25.944661   17772 kubeadm.go:587] restartPrimaryControlPlane start ...
I0606 15:02:25.962098   17772 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0606 15:02:26.045818   17772 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0606 15:02:26.055066   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0606 15:02:26.149812   17772 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:53276"
I0606 15:02:26.166354   17772 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0606 15:02:26.237803   17772 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0606 15:02:26.238309   17772 kubeadm.go:591] duration metric: took 293.6477ms to restartPrimaryControlPlane
I0606 15:02:26.238309   17772 kubeadm.go:393] duration metric: took 585.4091ms to StartCluster
I0606 15:02:26.238309   17772 settings.go:142] acquiring lock: {Name:mke5fed3a1ffc4e39fc02d56be041ef80d70f131 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0606 15:02:26.238309   17772 settings.go:150] Updating kubeconfig:  C:\Users\caleb\.kube\config
I0606 15:02:26.242057   17772 lock.go:35] WriteFile acquiring C:\Users\caleb\.kube\config: {Name:mkfdbd91911836666e35ae843e151ee2fd38f32c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0606 15:02:26.245456   17772 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0606 15:02:26.245456   17772 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0606 15:02:26.245964   17772 out.go:177] üîé  Verifying Kubernetes components...
I0606 15:02:26.245964   17772 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0606 15:02:26.245964   17772 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0606 15:02:26.245964   17772 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0606 15:02:26.245964   17772 addons.go:243] addon storage-provisioner should already be in state true
I0606 15:02:26.245964   17772 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0606 15:02:26.245964   17772 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0606 15:02:26.246550   17772 host.go:66] Checking if "minikube" exists ...
I0606 15:02:26.266328   17772 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0606 15:02:26.266328   17772 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 15:02:26.266843   17772 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0606 15:02:26.362283   17772 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0606 15:02:26.362283   17772 addons.go:243] addon default-storageclass should already be in state true
I0606 15:02:26.362283   17772 host.go:66] Checking if "minikube" exists ...
I0606 15:02:26.375865   17772 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0606 15:02:26.377932   17772 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0606 15:02:26.378453   17772 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0606 15:02:26.378453   17772 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0606 15:02:26.385747   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:26.469071   17772 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0606 15:02:26.469071   17772 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0606 15:02:26.475784   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 15:02:26.484589   17772 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53272 SSHKeyPath:C:\Users\caleb\.minikube\machines\minikube\id_rsa Username:docker}
I0606 15:02:26.575969   17772 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53272 SSHKeyPath:C:\Users\caleb\.minikube\machines\minikube\id_rsa Username:docker}
I0606 15:02:26.864322   17772 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0606 15:02:26.868004   17772 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0606 15:02:26.949369   17772 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0606 15:02:27.026251   17772 api_server.go:52] waiting for apiserver process to appear ...
I0606 15:02:27.048012   17772 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0606 15:02:27.059908   17772 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0606 15:02:29.338282   17772 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.2902694s)
I0606 15:02:29.338282   17772 api_server.go:72] duration metric: took 3.0928259s to wait for apiserver process to appear ...
I0606 15:02:29.338282   17772 api_server.go:88] waiting for apiserver healthz status ...
I0606 15:02:29.338282   17772 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53276/healthz ...
I0606 15:02:29.338282   17772 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.4702779s)
I0606 15:02:29.343008   17772 api_server.go:279] https://127.0.0.1:53276/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0606 15:02:29.343008   17772 api_server.go:103] status: https://127.0.0.1:53276/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0606 15:02:29.838344   17772 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53276/healthz ...
I0606 15:02:29.843977   17772 api_server.go:279] https://127.0.0.1:53276/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0606 15:02:29.843977   17772 api_server.go:103] status: https://127.0.0.1:53276/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0606 15:02:29.859449   17772 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.799541s)
I0606 15:02:29.860489   17772 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I0606 15:02:29.861007   17772 addons.go:505] duration metric: took 3.6155516s for enable addons: enabled=[default-storageclass storage-provisioner]
I0606 15:02:30.352312   17772 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53276/healthz ...
I0606 15:02:30.355949   17772 api_server.go:279] https://127.0.0.1:53276/healthz returned 200:
ok
I0606 15:02:30.356980   17772 api_server.go:141] control plane version: v1.30.0
I0606 15:02:30.356980   17772 api_server.go:131] duration metric: took 1.0186981s to wait for apiserver health ...
I0606 15:02:30.356980   17772 system_pods.go:43] waiting for kube-system pods to appear ...
I0606 15:02:30.361660   17772 system_pods.go:59] 7 kube-system pods found
I0606 15:02:30.361660   17772 system_pods.go:61] "coredns-7db6d8ff4d-s2696" [04b50871-42ee-4296-bfd2-161ac45cffac] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0606 15:02:30.361660   17772 system_pods.go:61] "etcd-minikube" [164d742d-1081-4b97-9849-b8f80f9c4e2b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0606 15:02:30.361660   17772 system_pods.go:61] "kube-apiserver-minikube" [3499f573-b42b-4404-b009-c828956862c1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0606 15:02:30.361660   17772 system_pods.go:61] "kube-controller-manager-minikube" [b45511a7-8719-4052-8a03-689b4d3eb322] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0606 15:02:30.361660   17772 system_pods.go:61] "kube-proxy-dffhk" [afafc5b3-4a2a-4cd6-8d7c-bc7ed1ca8e00] Running
I0606 15:02:30.361660   17772 system_pods.go:61] "kube-scheduler-minikube" [b4b0ffe0-bc0a-4b8b-86de-d12fedca9c06] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0606 15:02:30.361660   17772 system_pods.go:61] "storage-provisioner" [d91a6d14-0adb-4d0b-941d-254d81de96bc] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0606 15:02:30.361660   17772 system_pods.go:74] duration metric: took 4.6804ms to wait for pod list to return data ...
I0606 15:02:30.361660   17772 kubeadm.go:576] duration metric: took 4.1162044s to wait for: map[apiserver:true system_pods:true]
I0606 15:02:30.361660   17772 node_conditions.go:102] verifying NodePressure condition ...
I0606 15:02:30.364256   17772 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0606 15:02:30.364256   17772 node_conditions.go:123] node cpu capacity is 16
I0606 15:02:30.364256   17772 node_conditions.go:105] duration metric: took 2.5959ms to run NodePressure ...
I0606 15:02:30.364256   17772 start.go:240] waiting for startup goroutines ...
I0606 15:02:30.364256   17772 start.go:245] waiting for cluster config update ...
I0606 15:02:30.364256   17772 start.go:254] writing updated cluster config ...
I0606 15:02:30.379297   17772 ssh_runner.go:195] Run: rm -f paused
I0606 15:02:30.446961   17772 start.go:600] kubectl: 1.29.2, cluster: 1.30.0 (minor skew: 1)
I0606 15:02:30.447467   17772 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 06 22:02:23 minikube cri-dockerd[14966]: time="2024-06-06T22:02:23Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 06 22:02:23 minikube cri-dockerd[14966]: time="2024-06-06T22:02:23Z" level=info msg="Loaded network plugin cni"
Jun 06 22:02:23 minikube cri-dockerd[14966]: time="2024-06-06T22:02:23Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 06 22:02:23 minikube cri-dockerd[14966]: time="2024-06-06T22:02:23Z" level=info msg="Setting cgroupDriver cgroupfs"
Jun 06 22:02:23 minikube cri-dockerd[14966]: time="2024-06-06T22:02:23Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 06 22:02:23 minikube cri-dockerd[14966]: time="2024-06-06T22:02:23Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 06 22:02:23 minikube cri-dockerd[14966]: time="2024-06-06T22:02:23Z" level=info msg="Start cri-dockerd grpc backend"
Jun 06 22:02:23 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-receiver-7b585b6bb5-2s9rm_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a2587224328afaac8f34675488a4067dbc621c1be081e62b656aa40681d8daa2\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-receiver-7b585b6bb5-2s9rm_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6f60d71bdc944ec2adf60392b254db761c5edbe6c59de72e018891fe5354d97d\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-interface-5f9d59fcbb-cnwgl_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"75c851500fe79a8d538c81634ffaa16c49f452865202ce8e5cc600b8a2db5fb2\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-interface-5f9d59fcbb-cnwgl_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1df91ec90318b8e0b85b3058cae8798bdcab38e5a562aee52da891cdc20351af\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-broadcaster-576ff7fc69-z9twx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"057e4556c37355ce3d370e001b37ea9ead7a7293c9d5172e0975abbaaf7e08e9\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-broadcaster-576ff7fc69-z9twx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ea20dbe8937a5ef2d4923be4900e7fe69e9155b980b92b054a8e3244a1c97b58\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"rabbitmq-8575cd54c7-f654d_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"52cf31f71856415d1420eccd884ef5a5b824609f70c93f99bf3c37a9c4e1607f\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"rabbitmq-8575cd54c7-f654d_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ed5c94ba2102134d6c81f1250b11a329b529afa4585d163edd2009e801036997\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"rabbitmq-8575cd54c7-f654d_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f4e51d7ca745c9616a9a3c1da0282410d5e31b1d3ca3e338d9f8222ea5d59c8b\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-s2696_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5441424ff6ef83ac303260e5179439aae4a0ec668915eb97cd173b9249630574\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-s2696_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6435f65f7d599ed6fb6b3016cc32572ea2d7c7ac6bff9fefe2570ec4f7754f12\""
Jun 06 22:02:24 minikube cri-dockerd[14966]: time="2024-06-06T22:02:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-s2696_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fcf1358ef2a260f75e09c2911b4cb78e8d3d8954c9db536247384929458b5d31\""
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f1462d8ca79231f9284e109ffc68a9b31509cc8827fb8af167b2e01469506b97/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8184211e8b84d1596e05cc5bea33e9c9966aa433e1d23ef77d2be4a2a3ce0f23/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/839663c7f360b0cba8d72b01252eb2255d47a458aea8731dae05a589294d5b1f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6dfd142006013ea04cae88be407d949491fd8fbaa44bb08cd5840ffc6c4d956f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/86da1671a1d6512ed3ac0cbe69206d2268cd554fea92403511677c38858f536b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9316306b872ad1ff812cb45302f00f1125dbe1d31bfe787f1c403e939687d4eb/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d3a4d5b11ecdcee6c19bd69a008a5050654889bc2cb0d37e020a315209d067fa/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2885e9b6e551ff65a46cda7caf2a3aded724d12cb5a01effb430dee68acfa6c8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2228ec756db7afff9e6df3cb9e68401e6b600ce837bd4c5760b4df67862d5caf/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 06 22:02:25 minikube cri-dockerd[14966]: time="2024-06-06T22:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8b2b9b79040022a4a6f727c2324df9e069b47c9ce1fb6bc5c03b89c01e01ea52/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 06 22:02:26 minikube cri-dockerd[14966]: time="2024-06-06T22:02:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4e94bb165821e1594c75bd97b6cf971b0de40bad72da70fd591a3b6b41f89ab0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 06 22:02:26 minikube dockerd[14690]: time="2024-06-06T22:02:26.238460264Z" level=info msg="ignoring event" container=405f54856ccd30b514314c3e69cae7e80c1114534eb4e414ed22b17efc182d77 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 22:02:33 minikube dockerd[14690]: time="2024-06-06T22:02:33.259768828Z" level=info msg="ignoring event" container=058057a736c5adff71b47df36ae072116dc204892874ff7d9106df6b8eee8ebd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 22:10:49 minikube cri-dockerd[14966]: time="2024-06-06T22:10:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1d3efd7485fd8cdb47ec886423ea61a60a38f9cbba721ffb474203ac072b843a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 06 22:10:49 minikube cri-dockerd[14966]: time="2024-06-06T22:10:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3973e357358172ab730ab5ac59d8afc95213dca2d256254b40d79ce25f7782d8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 06 22:10:49 minikube cri-dockerd[14966]: time="2024-06-06T22:10:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c8c0d1cc87e851c66450f88b03c8fb7b71c0746f0e953074038529567521960e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 06 22:10:50 minikube dockerd[14690]: time="2024-06-06T22:10:50.605130865Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=ca969f91f54d9993 traceID=a087c68980eb44f3f8dce8f0ac571482
Jun 06 22:10:50 minikube dockerd[14690]: time="2024-06-06T22:10:50.605188167Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:10:51 minikube dockerd[14690]: time="2024-06-06T22:10:51.733726216Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=8bd4a2ea61bb98c0 traceID=5d6b66c79c4e2f28a22b418371065989
Jun 06 22:10:51 minikube dockerd[14690]: time="2024-06-06T22:10:51.733771917Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:10:52 minikube dockerd[14690]: time="2024-06-06T22:10:52.863971230Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=6a3990379bf53f9a traceID=616a0d6fc40c1512a3891195f70b532d
Jun 06 22:10:52 minikube dockerd[14690]: time="2024-06-06T22:10:52.864072734Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:11:04 minikube dockerd[14690]: time="2024-06-06T22:11:04.189235057Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=1e2c2d832ebc4935 traceID=11782f445340339c3e9a378f7885f333
Jun 06 22:11:04 minikube dockerd[14690]: time="2024-06-06T22:11:04.189283759Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:11:05 minikube dockerd[14690]: time="2024-06-06T22:11:05.319737321Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=06982ab418811e29 traceID=efe824612687e43a7ab85341f4ad6447
Jun 06 22:11:05 minikube dockerd[14690]: time="2024-06-06T22:11:05.319792123Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:11:06 minikube dockerd[14690]: time="2024-06-06T22:11:06.433109410Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=043ece59a0daeb62 traceID=28f1a886b09573f21ada61f1d1f7227a
Jun 06 22:11:06 minikube dockerd[14690]: time="2024-06-06T22:11:06.433327919Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:11:31 minikube dockerd[14690]: time="2024-06-06T22:11:31.220238467Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=658a9f7eba607f2b traceID=174a0d37239606eba7aece0c2b53c6cf
Jun 06 22:11:31 minikube dockerd[14690]: time="2024-06-06T22:11:31.220284669Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:11:32 minikube dockerd[14690]: time="2024-06-06T22:11:32.331598701Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=f55cf0c2dfd33eed traceID=bac2c61627024305323bd68cfeecbbe4
Jun 06 22:11:32 minikube dockerd[14690]: time="2024-06-06T22:11:32.331693905Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:11:35 minikube dockerd[14690]: time="2024-06-06T22:11:35.241171814Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=9ea2c1019af7ec82 traceID=c89a64ff3c28d1d517faf2a8cd2f8e71
Jun 06 22:11:35 minikube dockerd[14690]: time="2024-06-06T22:11:35.241225016Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:12:21 minikube dockerd[14690]: time="2024-06-06T22:12:21.191582694Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=ea0c7c111b2459bb traceID=caa6873949d8ba086b1b4b135372122a
Jun 06 22:12:21 minikube dockerd[14690]: time="2024-06-06T22:12:21.191634696Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:12:24 minikube dockerd[14690]: time="2024-06-06T22:12:24.172346273Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=c76a3905e22dc9b3 traceID=e9200c52bf9ee788a344a33da1aefd6d
Jun 06 22:12:24 minikube dockerd[14690]: time="2024-06-06T22:12:24.172398675Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 06 22:12:27 minikube dockerd[14690]: time="2024-06-06T22:12:27.185029751Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=9fed9d5822a5d839 traceID=aa86ad7fe17835a90273fc0d466f156c
Jun 06 22:12:27 minikube dockerd[14690]: time="2024-06-06T22:12:27.185083353Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
fd9cfa6215cab       07a0512835452       10 minutes ago      Running             hello-broadcaster         2                   2228ec756db7a       hello-broadcaster-576ff7fc69-z9twx
ae813acd93156       6e38f40d628db       10 minutes ago      Running             storage-provisioner       5                   86da1671a1d65       storage-provisioner
a970373ac51a5       cbb01a7bd410d       10 minutes ago      Running             coredns                   2                   4e94bb165821e       coredns-7db6d8ff4d-s2696
c19b583a25181       a4e86f36e8fdc       10 minutes ago      Running             rabbitmq                  2                   8b2b9b7904002       rabbitmq-8575cd54c7-f654d
058057a736c5a       07a0512835452       10 minutes ago      Exited              hello-broadcaster         1                   2228ec756db7a       hello-broadcaster-576ff7fc69-z9twx
f68dff8e781c5       a0bf559e280cf       10 minutes ago      Running             kube-proxy                2                   8184211e8b84d       kube-proxy-dffhk
8382c41bf477b       cc7567da7e214       10 minutes ago      Running             hello-receiver            1                   9316306b872ad       hello-receiver-7b585b6bb5-2s9rm
e3ad0e42bbb41       5f686809652e8       10 minutes ago      Running             web-interface             1                   d3a4d5b11ecdc       web-interface-5f9d59fcbb-cnwgl
faa693b0bcfdb       c7aad43836fa5       10 minutes ago      Running             kube-controller-manager   2                   2885e9b6e551f       kube-controller-manager-minikube
405f54856ccd3       6e38f40d628db       10 minutes ago      Exited              storage-provisioner       4                   86da1671a1d65       storage-provisioner
f4a9d4805308d       c42f13656d0b2       10 minutes ago      Running             kube-apiserver            2                   6dfd142006013       kube-apiserver-minikube
2e5740ef4b5ad       3861cfcd7c04c       10 minutes ago      Running             etcd                      2                   f1462d8ca7923       etcd-minikube
816a6609607c3       259c8277fcbbc       10 minutes ago      Running             kube-scheduler            2                   839663c7f360b       kube-scheduler-minikube
1ccbd12ce7332       a4e86f36e8fdc       28 minutes ago      Exited              rabbitmq                  1                   ed5c94ba21021       rabbitmq-8575cd54c7-f654d
8b36c66e92188       cbb01a7bd410d       28 minutes ago      Exited              coredns                   1                   6435f65f7d599       coredns-7db6d8ff4d-s2696
b0e3c44cd7b4a       c42f13656d0b2       28 minutes ago      Exited              kube-apiserver            1                   c58cea35e02d8       kube-apiserver-minikube
3f0123d49e7e9       a0bf559e280cf       28 minutes ago      Exited              kube-proxy                1                   9da0259b6af3e       kube-proxy-dffhk
69d3c3800c43c       c7aad43836fa5       28 minutes ago      Exited              kube-controller-manager   1                   87ac04853171e       kube-controller-manager-minikube
3eb1eb27ee264       259c8277fcbbc       28 minutes ago      Exited              kube-scheduler            1                   6b686f3626fd7       kube-scheduler-minikube
b35925344adc0       3861cfcd7c04c       28 minutes ago      Exited              etcd                      1                   4e7a3842b4a18       etcd-minikube


==> coredns [8b36c66e9218] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:50846 - 47179 "HINFO IN 1534360935725292289.2005836814796421845. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.16853708s
[INFO] 10.244.0.9:44695 - 48028 "A IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000152805s
[INFO] 10.244.0.9:44695 - 1426 "AAAA IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 145 0.000242307s
[INFO] 10.244.0.7:56909 - 60235 "AAAA IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 145 0.000113404s
[INFO] 10.244.0.7:56909 - 31049 "A IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000144104s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [a970373ac51a] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "endpointslices" in API group "discovery.k8s.io" at the cluster scope
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "endpointslices" in API group "discovery.k8s.io" at the cluster scope
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: services is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "services" in API group "" at the cluster scope
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "services" in API group "" at the cluster scope
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: namespaces is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "namespaces" in API group "" at the cluster scope
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "namespaces" in API group "" at the cluster scope
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:52013 - 14943 "HINFO IN 4648992820754608265.1006105475681194499. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.222544386s
[INFO] 10.244.0.14:57482 - 44240 "AAAA IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 145 0.000149704s
[INFO] 10.244.0.14:57482 - 36819 "A IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000187705s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_06_06T14_40_26_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 06 Jun 2024 21:40:23 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 06 Jun 2024 22:13:12 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 06 Jun 2024 22:10:41 +0000   Thu, 06 Jun 2024 21:40:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 06 Jun 2024 22:10:41 +0000   Thu, 06 Jun 2024 21:40:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 06 Jun 2024 22:10:41 +0000   Thu, 06 Jun 2024 21:40:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 06 Jun 2024 22:10:41 +0000   Thu, 06 Jun 2024 21:40:23 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8036772Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8036772Ki
  pods:               110
System Info:
  Machine ID:                 102b44c8de4e4c0d8b9724e1e6d09964
  System UUID:                102b44c8de4e4c0d8b9724e1e6d09964
  Boot ID:                    46783d7e-e661-4384-aed6-b404992aba3c
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                             CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                             ------------  ----------  ---------------  -------------  ---
  default                     hello-broadcaster-576ff7fc69-z9twx               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29m
  default                     hello-broadcaster-deployment-6688cf76bb-9crvj    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m33s
  default                     hello-receiver-7b585b6bb5-2s9rm                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29m
  default                     hello-receiver-deployment-5f5d76b494-tjn6v       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m33s
  default                     rabbitmq-8575cd54c7-f654d                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29m
  default                     web-interface-5f9d59fcbb-cnwgl                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29m
  default                     web-service-deployment-586f94df95-t8zcw          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m33s
  kube-system                 coredns-7db6d8ff4d-s2696                         100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     32m
  kube-system                 etcd-minikube                                    100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         32m
  kube-system                 kube-apiserver-minikube                          250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
  kube-system                 kube-controller-manager-minikube                 200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
  kube-system                 kube-proxy-dffhk                                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
  kube-system                 kube-scheduler-minikube                          100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
  kube-system                 storage-provisioner                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 32m                kube-proxy       
  Normal  Starting                 10m                kube-proxy       
  Normal  Starting                 28m                kube-proxy       
  Normal  NodeAllocatableEnforced  33m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasNoDiskPressure    33m (x8 over 33m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     33m (x7 over 33m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  33m (x8 over 33m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  Starting                 32m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  32m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  32m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    32m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     32m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           32m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  RegisteredNode           28m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  RegisteredNode           10m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.012669] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000189] netlink: 'init': attribute type 4 has an invalid length.
[  +0.000129] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000066] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000063] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000058] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000055] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000082] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000059] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000061] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000335] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000394] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000385] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000354] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000341] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000348] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000434] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000257] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000352] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000363] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000316] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000354] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000282] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002700] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000405] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000426] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000412] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.028845] /sbin/ldconfig.real: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.016627] FS-Cache: Duplicate cookie detected
[  +0.000376] FS-Cache: O-cookie c=00000025 [p=00000002 fl=222 nc=0 na=1]
[  +0.000439] FS-Cache: O-cookie d=00000000b14ae764{9P.session} n=000000008558baa0
[  +0.000421] FS-Cache: O-key=[10] '34323934393337353834'
[  +0.000318] FS-Cache: N-cookie c=00000027 [p=00000002 fl=2 nc=0 na=1]
[  +0.000338] FS-Cache: N-cookie d=00000000b14ae764{9P.session} n=00000000afa98aa2
[  +0.000406] FS-Cache: N-key=[10] '34323934393337353834'
[  +0.011608] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000581] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000524] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000575] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.008144] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.011057] new mount options do not match the existing superblock, will be ignored
[  +0.062897] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000661] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000437] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000469] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000547] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000473] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000470] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000521] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000505] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001681] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000757] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000522] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000439] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.109185] systemd-journald[37]: File /var/log/journal/bae55768ffb34140bf79aa1dbc470bd1/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.034117] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000248] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003987] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2


==> etcd [2e5740ef4b5a] <==
{"level":"warn","ts":"2024-06-06T22:02:26.245396Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-06T22:02:26.245549Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-06-06T22:02:26.245629Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-06-06T22:02:26.245681Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-06T22:02:26.245697Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-06T22:02:26.245756Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-06T22:02:26.246702Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-06-06T22:02:26.24685Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-06-06T22:02:26.249097Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.015357ms"}
{"level":"info","ts":"2024-06-06T22:02:26.342769Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-06-06T22:02:26.436998Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":1977}
{"level":"info","ts":"2024-06-06T22:02:26.437097Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-06-06T22:02:26.437119Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2024-06-06T22:02:26.437129Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 3, commit: 1977, applied: 0, lastindex: 1977, lastterm: 3]"}
{"level":"warn","ts":"2024-06-06T22:02:26.441006Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-06-06T22:02:26.442759Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1350}
{"level":"info","ts":"2024-06-06T22:02:26.444348Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1700}
{"level":"info","ts":"2024-06-06T22:02:26.446436Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-06-06T22:02:26.449357Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-06-06T22:02:26.44979Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-06T22:02:26.449821Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-06-06T22:02:26.45005Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-06-06T22:02:26.450219Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-06T22:02:26.450261Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-06T22:02:26.450287Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-06T22:02:26.450447Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-06-06T22:02:26.450573Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-06T22:02:26.450724Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-06T22:02:26.450837Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-06T22:02:26.453063Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-06T22:02:26.454554Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-06-06T22:02:26.536168Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-06-06T22:02:26.536251Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-06T22:02:26.536259Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-06T22:02:27.939674Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2024-06-06T22:02:27.939728Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2024-06-06T22:02:27.939742Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-06-06T22:02:27.939752Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2024-06-06T22:02:27.939758Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-06-06T22:02:27.939765Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2024-06-06T22:02:27.939788Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-06-06T22:02:27.950395Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-06-06T22:02:27.950406Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-06T22:02:27.950418Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-06T22:02:27.951206Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-06-06T22:02:27.951234Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-06-06T22:02:27.951982Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-06-06T22:02:27.952048Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-06-06T22:12:27.946482Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2054}
{"level":"info","ts":"2024-06-06T22:12:27.963797Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2054,"took":"17.11566ms","hash":2217513156,"current-db-size-bytes":3137536,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1871872,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-06-06T22:12:27.963839Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2217513156,"revision":2054,"compact-revision":1350}


==> etcd [b35925344adc] <==
{"level":"warn","ts":"2024-06-06T21:44:52.783308Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-06T21:44:52.783338Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-06T21:44:52.783389Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-06T21:44:52.783944Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-06-06T21:44:52.784052Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-06-06T21:44:52.787392Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"3.170704ms"}
{"level":"info","ts":"2024-06-06T21:44:52.866143Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-06-06T21:44:52.871024Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":803}
{"level":"info","ts":"2024-06-06T21:44:52.871126Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-06-06T21:44:52.871152Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2024-06-06T21:44:52.871173Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 803, applied: 0, lastindex: 803, lastterm: 2]"}
{"level":"warn","ts":"2024-06-06T21:44:52.87279Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-06-06T21:44:52.877166Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":740}
{"level":"info","ts":"2024-06-06T21:44:52.879078Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-06-06T21:44:52.881847Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-06-06T21:44:52.882253Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-06T21:44:52.882284Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-06-06T21:44:52.882527Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-06-06T21:44:52.882565Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-06T21:44:52.882614Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-06T21:44:52.882623Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-06T21:44:52.883067Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-06-06T21:44:52.883124Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-06T21:44:52.883191Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-06T21:44:52.883217Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-06T21:44:52.884613Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-06T21:44:52.884695Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-06T21:44:52.884712Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-06T21:44:52.88481Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-06-06T21:44:52.884831Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-06-06T21:44:54.471699Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2024-06-06T21:44:54.471749Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2024-06-06T21:44:54.471771Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-06-06T21:44:54.471781Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-06-06T21:44:54.471785Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-06-06T21:44:54.471797Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-06-06T21:44:54.471802Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-06-06T21:44:54.472645Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-06-06T21:44:54.472715Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-06T21:44:54.472756Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-06T21:44:54.47291Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-06-06T21:44:54.472947Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-06-06T21:44:54.47407Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-06-06T21:44:54.474489Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-06-06T21:54:54.4816Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1111}
{"level":"info","ts":"2024-06-06T21:54:54.500441Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1111,"took":"18.440836ms","hash":244124365,"current-db-size-bytes":3125248,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1380352,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-06T21:54:54.500487Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":244124365,"revision":1111,"compact-revision":-1}
{"level":"info","ts":"2024-06-06T21:59:54.487311Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1350}
{"level":"info","ts":"2024-06-06T21:59:54.489171Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1350,"took":"1.727853ms","hash":3906913670,"current-db-size-bytes":3125248,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1638400,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-06-06T21:59:54.489206Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3906913670,"revision":1350,"compact-revision":1111}
{"level":"info","ts":"2024-06-06T22:02:10.592638Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-06-06T22:02:10.592706Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-06-06T22:02:10.592789Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-06T22:02:10.592904Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-06T22:02:10.73609Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-06T22:02:10.736157Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-06-06T22:02:10.7376Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-06T22:02:10.838837Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-06T22:02:10.839005Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-06T22:02:10.839053Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 22:13:21 up 45 min,  0 users,  load average: 0.22, 0.26, 0.26
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [b0e3c44cd7b4] <==
W0606 22:02:16.250946       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:16.258180       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:16.280209       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:16.313166       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:16.340957       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:16.366950       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:16.376243       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:16.445021       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:18.701652       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:18.782396       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:18.817029       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:18.847397       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:18.957671       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.060244       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.127652       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.138799       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.331479       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.387085       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.440586       1 logging.go:59] [core] [Channel #2 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.463325       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.482288       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.500652       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.501965       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.532713       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.554428       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.586130       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.586130       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.593632       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.600048       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.601500       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.604988       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.614153       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.659635       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.731879       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.780243       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.782754       1 logging.go:59] [core] [Channel #1 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.786182       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.789500       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.819872       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:19.937948       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.000221       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.000376       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.029519       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.036403       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.047142       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.058320       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.063015       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.106518       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.121025       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.192587       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.284288       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.307833       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.418060       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.422145       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.447266       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.453333       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.510097       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.527439       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.603629       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0606 22:02:20.718302       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [f4a9d4805308] <==
W0606 22:02:28.580900       1 genericapiserver.go:733] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0606 22:02:28.587548       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0606 22:02:28.587577       1 genericapiserver.go:733] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0606 22:02:28.990381       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0606 22:02:28.990517       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0606 22:02:28.990535       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0606 22:02:28.990713       1 secure_serving.go:213] Serving securely on [::]:8443
I0606 22:02:28.990793       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0606 22:02:28.990831       1 aggregator.go:163] waiting for initial CRD sync...
I0606 22:02:28.990968       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0606 22:02:28.990975       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0606 22:02:28.991115       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0606 22:02:28.991220       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0606 22:02:28.991275       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0606 22:02:28.991325       1 controller.go:78] Starting OpenAPI AggregationController
I0606 22:02:28.991375       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0606 22:02:28.991489       1 controller.go:139] Starting OpenAPI controller
I0606 22:02:28.991536       1 controller.go:87] Starting OpenAPI V3 controller
I0606 22:02:28.991549       1 naming_controller.go:291] Starting NamingConditionController
I0606 22:02:28.991558       1 establishing_controller.go:76] Starting EstablishingController
I0606 22:02:28.991570       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0606 22:02:28.991580       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0606 22:02:28.991588       1 crd_finalizer.go:266] Starting CRDFinalizer
I0606 22:02:28.991079       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0606 22:02:28.991596       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0606 22:02:28.992011       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0606 22:02:28.992023       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0606 22:02:28.992055       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0606 22:02:28.992093       1 controller.go:116] Starting legacy_token_tracking_controller
I0606 22:02:28.992097       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0606 22:02:28.992139       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0606 22:02:28.992182       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0606 22:02:28.992226       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0606 22:02:28.992963       1 available_controller.go:423] Starting AvailableConditionController
I0606 22:02:28.992987       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0606 22:02:29.041589       1 shared_informer.go:320] Caches are synced for node_authorizer
I0606 22:02:29.045280       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0606 22:02:29.045314       1 policy_source.go:224] refreshing policies
I0606 22:02:29.135707       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0606 22:02:29.135774       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0606 22:02:29.137781       1 shared_informer.go:320] Caches are synced for configmaps
I0606 22:02:29.135703       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0606 22:02:29.137851       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0606 22:02:29.137859       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0606 22:02:29.137874       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0606 22:02:29.137875       1 aggregator.go:165] initial CRD sync complete...
I0606 22:02:29.137887       1 autoregister_controller.go:141] Starting autoregister controller
I0606 22:02:29.137895       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0606 22:02:29.137905       1 cache.go:39] Caches are synced for autoregister controller
I0606 22:02:29.139318       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0606 22:02:29.146505       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0606 22:02:29.148910       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0606 22:02:29.994918       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0606 22:02:41.326181       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0606 22:02:41.390151       1 controller.go:615] quota admission added evaluator for: endpoints
I0606 22:02:41.390151       1 controller.go:615] quota admission added evaluator for: endpoints
I0606 22:10:48.768771       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0606 22:10:48.773405       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0606 22:10:48.791105       1 alloc.go:330] "allocated clusterIPs" service="default/hello-receiver-service" clusterIPs={"IPv4":"10.102.53.224"}
I0606 22:10:48.817351       1 alloc.go:330] "allocated clusterIPs" service="default/web-service-service" clusterIPs={"IPv4":"10.101.233.16"}


==> kube-controller-manager [69d3c3800c43] <==
I0606 21:45:07.275299       1 shared_informer.go:320] Caches are synced for job
I0606 21:45:07.275408       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0606 21:45:07.276700       1 shared_informer.go:320] Caches are synced for deployment
I0606 21:45:07.276949       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0606 21:45:07.281350       1 shared_informer.go:320] Caches are synced for HPA
I0606 21:45:07.285092       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0606 21:45:07.285095       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0606 21:45:07.286233       1 shared_informer.go:320] Caches are synced for taint
I0606 21:45:07.286279       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0606 21:45:07.286309       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0606 21:45:07.286358       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0606 21:45:07.288191       1 shared_informer.go:320] Caches are synced for namespace
I0606 21:45:07.292706       1 shared_informer.go:320] Caches are synced for node
I0606 21:45:07.292741       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0606 21:45:07.292752       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0606 21:45:07.292755       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0606 21:45:07.292758       1 shared_informer.go:320] Caches are synced for cidrallocator
I0606 21:45:07.293851       1 shared_informer.go:320] Caches are synced for PV protection
I0606 21:45:07.296022       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0606 21:45:07.296092       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="28.5¬µs"
I0606 21:45:07.296135       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/rabbitmq-8575cd54c7" duration="14.701¬µs"
I0606 21:45:07.296159       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="17.4¬µs"
I0606 21:45:07.296158       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-interface-5f9d59fcbb" duration="17.7¬µs"
I0606 21:45:07.296165       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-7b585b6bb5" duration="41.801¬µs"
I0606 21:45:07.302722       1 shared_informer.go:320] Caches are synced for crt configmap
I0606 21:45:07.304928       1 shared_informer.go:320] Caches are synced for endpoint
I0606 21:45:07.306186       1 shared_informer.go:320] Caches are synced for stateful set
I0606 21:45:07.320594       1 shared_informer.go:320] Caches are synced for service account
I0606 21:45:07.321793       1 shared_informer.go:320] Caches are synced for daemon sets
I0606 21:45:07.325159       1 shared_informer.go:320] Caches are synced for GC
I0606 21:45:07.334982       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0606 21:45:07.336122       1 shared_informer.go:320] Caches are synced for attach detach
I0606 21:45:07.336153       1 shared_informer.go:320] Caches are synced for TTL after finished
I0606 21:45:07.369429       1 shared_informer.go:320] Caches are synced for persistent volume
I0606 21:45:07.370531       1 shared_informer.go:320] Caches are synced for expand
I0606 21:45:07.371725       1 shared_informer.go:320] Caches are synced for ephemeral
I0606 21:45:07.373978       1 shared_informer.go:320] Caches are synced for PVC protection
I0606 21:45:07.386803       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0606 21:45:07.399788       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0606 21:45:07.399806       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0606 21:45:07.400917       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0606 21:45:07.401035       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0606 21:45:07.478940       1 shared_informer.go:320] Caches are synced for ReplicationController
I0606 21:45:07.498365       1 shared_informer.go:320] Caches are synced for disruption
I0606 21:45:07.519408       1 shared_informer.go:320] Caches are synced for resource quota
I0606 21:45:07.523867       1 shared_informer.go:320] Caches are synced for cronjob
I0606 21:45:07.538010       1 shared_informer.go:320] Caches are synced for resource quota
I0606 21:45:07.939505       1 shared_informer.go:320] Caches are synced for garbage collector
I0606 21:45:07.939533       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0606 21:45:07.968688       1 shared_informer.go:320] Caches are synced for garbage collector
I0606 21:45:10.094744       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="32.001¬µs"
I0606 21:45:18.101581       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-interface-5f9d59fcbb" duration="24.301¬µs"
I0606 21:45:22.090272       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-7b585b6bb5" duration="26.201¬µs"
I0606 21:45:37.094356       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-7b585b6bb5" duration="27.4¬µs"
I0606 21:45:44.032583       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="7.989734ms"
I0606 21:45:44.032698       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="25.901¬µs"
I0606 21:45:48.067910       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-interface-5f9d59fcbb" duration="3.816711ms"
I0606 21:45:48.067970       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-interface-5f9d59fcbb" duration="19.5¬µs"
I0606 21:45:51.089438       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-7b585b6bb5" duration="3.797912ms"
I0606 21:45:51.089499       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-7b585b6bb5" duration="21.301¬µs"


==> kube-controller-manager [faa693b0bcfd] <==
I0606 22:02:41.308043       1 shared_informer.go:320] Caches are synced for expand
I0606 22:02:41.308156       1 shared_informer.go:320] Caches are synced for crt configmap
I0606 22:02:41.315341       1 shared_informer.go:320] Caches are synced for namespace
I0606 22:02:41.316604       1 shared_informer.go:320] Caches are synced for cronjob
I0606 22:02:41.317780       1 shared_informer.go:320] Caches are synced for job
I0606 22:02:41.318979       1 shared_informer.go:320] Caches are synced for deployment
I0606 22:02:41.319892       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0606 22:02:41.320853       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="15.310521ms"
I0606 22:02:41.320935       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0606 22:02:41.320958       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="24.901¬µs"
I0606 22:02:41.322302       1 shared_informer.go:320] Caches are synced for daemon sets
I0606 22:02:41.324792       1 shared_informer.go:320] Caches are synced for persistent volume
I0606 22:02:41.326162       1 shared_informer.go:320] Caches are synced for service account
I0606 22:02:41.385548       1 shared_informer.go:320] Caches are synced for endpoint
I0606 22:02:41.429482       1 shared_informer.go:320] Caches are synced for resource quota
I0606 22:02:41.447251       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0606 22:02:41.477271       1 shared_informer.go:320] Caches are synced for resource quota
I0606 22:02:41.503377       1 shared_informer.go:320] Caches are synced for attach detach
I0606 22:02:41.937663       1 shared_informer.go:320] Caches are synced for garbage collector
I0606 22:02:41.988620       1 shared_informer.go:320] Caches are synced for garbage collector
I0606 22:02:41.988695       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0606 22:02:44.063747       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="31.901¬µs"
I0606 22:02:44.550193       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="8.964846ms"
I0606 22:02:44.550351       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-576ff7fc69" duration="58.302¬µs"
I0606 22:10:48.785998       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="11.111332ms"
I0606 22:10:48.791054       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="10.132793ms"
I0606 22:10:48.791107       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="5.075898ms"
I0606 22:10:48.791160       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="33.202¬µs"
I0606 22:10:48.795840       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="4.727483ms"
I0606 22:10:48.795916       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="30.101¬µs"
I0606 22:10:48.798019       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="74.403¬µs"
I0606 22:10:48.804392       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="25.001¬µs"
I0606 22:10:48.812865       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="7.163779ms"
I0606 22:10:48.817246       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="4.335668ms"
I0606 22:10:48.817307       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="27.001¬µs"
I0606 22:10:48.822473       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="21.901¬µs"
I0606 22:10:51.434668       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="52.302¬µs"
I0606 22:10:52.411655       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="32.802¬µs"
I0606 22:10:53.429998       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="88.303¬µs"
I0606 22:11:03.053411       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="49.002¬µs"
I0606 22:11:04.044816       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="42.702¬µs"
I0606 22:11:05.054942       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="25.501¬µs"
I0606 22:11:18.048694       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="56.603¬µs"
I0606 22:11:18.058628       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="43.302¬µs"
I0606 22:11:19.056189       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="58.203¬µs"
I0606 22:11:30.053236       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="103.904¬µs"
I0606 22:11:30.067520       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="88.004¬µs"
I0606 22:11:34.044510       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="37.001¬µs"
I0606 22:11:44.059910       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="47.502¬µs"
I0606 22:11:45.053717       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="51.602¬µs"
I0606 22:11:47.043876       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="37.602¬µs"
I0606 22:11:55.069978       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="83.504¬µs"
I0606 22:11:58.047311       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="38.001¬µs"
I0606 22:11:58.057209       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="58.202¬µs"
I0606 22:12:36.043185       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="59.402¬µs"
I0606 22:12:37.041444       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="30.701¬µs"
I0606 22:12:42.051413       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="44.601¬µs"
I0606 22:12:47.042624       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-receiver-deployment-5f5d76b494" duration="34.601¬µs"
I0606 22:12:48.052173       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-broadcaster-deployment-6688cf76bb" duration="32.901¬µs"
I0606 22:12:56.051218       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-service-deployment-586f94df95" duration="38.501¬µs"


==> kube-proxy [3f0123d49e7e] <==
I0606 21:44:53.269643       1 server_linux.go:69] "Using iptables proxy"
E0606 21:44:53.271501       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0606 21:44:55.471741       1 server.go:1051] "Failed to retrieve node info" err="nodes \"minikube\" is forbidden: User \"system:serviceaccount:kube-system:kube-proxy\" cannot get resource \"nodes\" in API group \"\" at the cluster scope"
I0606 21:44:57.691690       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0606 21:44:57.706685       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0606 21:44:57.706725       1 server_linux.go:165] "Using iptables Proxier"
I0606 21:44:57.708070       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0606 21:44:57.708093       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0606 21:44:57.708110       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0606 21:44:57.708337       1 server.go:872] "Version info" version="v1.30.0"
I0606 21:44:57.708369       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0606 21:44:57.709028       1 config.go:192] "Starting service config controller"
I0606 21:44:57.709058       1 shared_informer.go:313] Waiting for caches to sync for service config
I0606 21:44:57.709051       1 config.go:101] "Starting endpoint slice config controller"
I0606 21:44:57.709081       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0606 21:44:57.709154       1 config.go:319] "Starting node config controller"
I0606 21:44:57.709183       1 shared_informer.go:313] Waiting for caches to sync for node config
I0606 21:44:57.809655       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0606 21:44:57.809694       1 shared_informer.go:320] Caches are synced for node config
I0606 21:44:57.809823       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [f68dff8e781c] <==
I0606 22:02:26.242466       1 server_linux.go:69] "Using iptables proxy"
E0606 22:02:26.244908       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0606 22:02:29.043589       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0606 22:02:29.242289       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0606 22:02:29.242369       1 server_linux.go:165] "Using iptables Proxier"
I0606 22:02:29.244279       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0606 22:02:29.244316       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0606 22:02:29.244335       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0606 22:02:29.244624       1 server.go:872] "Version info" version="v1.30.0"
I0606 22:02:29.244679       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0606 22:02:29.245575       1 config.go:101] "Starting endpoint slice config controller"
I0606 22:02:29.245621       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0606 22:02:29.245652       1 config.go:192] "Starting service config controller"
I0606 22:02:29.245658       1 shared_informer.go:313] Waiting for caches to sync for service config
I0606 22:02:29.246181       1 config.go:319] "Starting node config controller"
I0606 22:02:29.246217       1 shared_informer.go:313] Waiting for caches to sync for node config
I0606 22:02:29.345798       1 shared_informer.go:320] Caches are synced for service config
I0606 22:02:29.345814       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0606 22:02:29.346395       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [3eb1eb27ee26] <==
I0606 21:44:53.785845       1 serving.go:380] Generated self-signed cert in-memory
W0606 21:44:55.371462       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0606 21:44:55.371495       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0606 21:44:55.371507       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0606 21:44:55.371516       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0606 21:44:55.467259       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0606 21:44:55.467282       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0606 21:44:55.469113       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0606 21:44:55.469164       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0606 21:44:55.469832       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0606 21:44:55.470212       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0606 21:44:55.570021       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0606 22:02:10.593134       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0606 22:02:10.593177       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0606 22:02:10.593278       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0606 22:02:10.593364       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [816a6609607c] <==
I0606 22:02:27.740951       1 serving.go:380] Generated self-signed cert in-memory
I0606 22:02:29.144046       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0606 22:02:29.144110       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0606 22:02:29.150691       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0606 22:02:29.150755       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0606 22:02:29.150763       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0606 22:02:29.150775       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0606 22:02:29.150694       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0606 22:02:29.150844       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0606 22:02:29.150936       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0606 22:02:29.150974       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0606 22:02:29.251716       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0606 22:02:29.251812       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0606 22:02:29.251899       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController


==> kubelet <==
Jun 06 22:10:53 minikube kubelet[2474]: E0606 22:10:53.420923    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:11:04 minikube kubelet[2474]: E0606 22:11:04.191593    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="web_service:latest"
Jun 06 22:11:04 minikube kubelet[2474]: E0606 22:11:04.191638    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="web_service:latest"
Jun 06 22:11:04 minikube kubelet[2474]: E0606 22:11:04.191767    2474 kuberuntime_manager.go:1256] container &Container{Name:web-service,Image:web_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4mpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod web-service-deployment-586f94df95-t8zcw_default(dba7a263-0971-452c-b209-d97b9011b4cf): ErrImagePull: Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:11:04 minikube kubelet[2474]: E0606 22:11:04.191787    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ErrImagePull: \"Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:11:05 minikube kubelet[2474]: E0606 22:11:05.332532    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_broadcaster:latest"
Jun 06 22:11:05 minikube kubelet[2474]: E0606 22:11:05.332574    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_broadcaster:latest"
Jun 06 22:11:05 minikube kubelet[2474]: E0606 22:11:05.332690    2474 kuberuntime_manager.go:1256] container &Container{Name:hello-broadcaster,Image:hello_broadcaster:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8w8k2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-broadcaster-deployment-6688cf76bb-9crvj_default(a9be3daa-af42-4944-8e2a-88733bb8c12d): ErrImagePull: Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:11:05 minikube kubelet[2474]: E0606 22:11:05.332708    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ErrImagePull: \"Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:11:06 minikube kubelet[2474]: E0606 22:11:06.449713    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_receiver:latest"
Jun 06 22:11:06 minikube kubelet[2474]: E0606 22:11:06.449867    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_receiver:latest"
Jun 06 22:11:06 minikube kubelet[2474]: E0606 22:11:06.450091    2474 kuberuntime_manager.go:1256] container &Container{Name:hello-receiver,Image:hello_receiver:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5001,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-receiver-deployment-5f5d76b494-tjn6v_default(99b5d400-5895-4c01-902f-22ce80bba609): ErrImagePull: Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:11:06 minikube kubelet[2474]: E0606 22:11:06.450151    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ErrImagePull: \"Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:11:18 minikube kubelet[2474]: E0606 22:11:18.040086    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_broadcaster:latest\\\"\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:11:18 minikube kubelet[2474]: E0606 22:11:18.040216    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:11:19 minikube kubelet[2474]: E0606 22:11:19.039802    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ImagePullBackOff: \"Back-off pulling image \\\"web_service:latest\\\"\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:11:31 minikube kubelet[2474]: E0606 22:11:31.233066    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_receiver:latest"
Jun 06 22:11:31 minikube kubelet[2474]: E0606 22:11:31.233112    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_receiver:latest"
Jun 06 22:11:31 minikube kubelet[2474]: E0606 22:11:31.233244    2474 kuberuntime_manager.go:1256] container &Container{Name:hello-receiver,Image:hello_receiver:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5001,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-receiver-deployment-5f5d76b494-tjn6v_default(99b5d400-5895-4c01-902f-22ce80bba609): ErrImagePull: Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:11:31 minikube kubelet[2474]: E0606 22:11:31.233262    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ErrImagePull: \"Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:11:32 minikube kubelet[2474]: E0606 22:11:32.345654    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_broadcaster:latest"
Jun 06 22:11:32 minikube kubelet[2474]: E0606 22:11:32.345754    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_broadcaster:latest"
Jun 06 22:11:32 minikube kubelet[2474]: E0606 22:11:32.345893    2474 kuberuntime_manager.go:1256] container &Container{Name:hello-broadcaster,Image:hello_broadcaster:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8w8k2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-broadcaster-deployment-6688cf76bb-9crvj_default(a9be3daa-af42-4944-8e2a-88733bb8c12d): ErrImagePull: Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:11:32 minikube kubelet[2474]: E0606 22:11:32.345930    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ErrImagePull: \"Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:11:35 minikube kubelet[2474]: E0606 22:11:35.243784    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="web_service:latest"
Jun 06 22:11:35 minikube kubelet[2474]: E0606 22:11:35.243829    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="web_service:latest"
Jun 06 22:11:35 minikube kubelet[2474]: E0606 22:11:35.243898    2474 kuberuntime_manager.go:1256] container &Container{Name:web-service,Image:web_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4mpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod web-service-deployment-586f94df95-t8zcw_default(dba7a263-0971-452c-b209-d97b9011b4cf): ErrImagePull: Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:11:35 minikube kubelet[2474]: E0606 22:11:35.243915    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ErrImagePull: \"Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:11:44 minikube kubelet[2474]: E0606 22:11:44.039638    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:11:45 minikube kubelet[2474]: E0606 22:11:45.038041    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_broadcaster:latest\\\"\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:11:47 minikube kubelet[2474]: E0606 22:11:47.038668    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ImagePullBackOff: \"Back-off pulling image \\\"web_service:latest\\\"\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:11:55 minikube kubelet[2474]: E0606 22:11:55.043684    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:11:58 minikube kubelet[2474]: E0606 22:11:58.039133    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_broadcaster:latest\\\"\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:11:58 minikube kubelet[2474]: E0606 22:11:58.039149    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ImagePullBackOff: \"Back-off pulling image \\\"web_service:latest\\\"\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:12:09 minikube kubelet[2474]: E0606 22:12:09.037451    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:12:10 minikube kubelet[2474]: E0606 22:12:10.036822    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_broadcaster:latest\\\"\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:12:12 minikube kubelet[2474]: E0606 22:12:12.036431    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ImagePullBackOff: \"Back-off pulling image \\\"web_service:latest\\\"\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:12:21 minikube kubelet[2474]: E0606 22:12:21.194338    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_receiver:latest"
Jun 06 22:12:21 minikube kubelet[2474]: E0606 22:12:21.194385    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_receiver:latest"
Jun 06 22:12:21 minikube kubelet[2474]: E0606 22:12:21.194477    2474 kuberuntime_manager.go:1256] container &Container{Name:hello-receiver,Image:hello_receiver:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5001,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-receiver-deployment-5f5d76b494-tjn6v_default(99b5d400-5895-4c01-902f-22ce80bba609): ErrImagePull: Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:12:21 minikube kubelet[2474]: E0606 22:12:21.194495    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ErrImagePull: \"Error response from daemon: pull access denied for hello_receiver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:12:24 minikube kubelet[2474]: E0606 22:12:24.185102    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_broadcaster:latest"
Jun 06 22:12:24 minikube kubelet[2474]: E0606 22:12:24.185146    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hello_broadcaster:latest"
Jun 06 22:12:24 minikube kubelet[2474]: E0606 22:12:24.185212    2474 kuberuntime_manager.go:1256] container &Container{Name:hello-broadcaster,Image:hello_broadcaster:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8w8k2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-broadcaster-deployment-6688cf76bb-9crvj_default(a9be3daa-af42-4944-8e2a-88733bb8c12d): ErrImagePull: Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:12:24 minikube kubelet[2474]: E0606 22:12:24.185230    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ErrImagePull: \"Error response from daemon: pull access denied for hello_broadcaster, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:12:27 minikube kubelet[2474]: E0606 22:12:27.197280    2474 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="web_service:latest"
Jun 06 22:12:27 minikube kubelet[2474]: E0606 22:12:27.197347    2474 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="web_service:latest"
Jun 06 22:12:27 minikube kubelet[2474]: E0606 22:12:27.197418    2474 kuberuntime_manager.go:1256] container &Container{Name:web-service,Image:web_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4mpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod web-service-deployment-586f94df95-t8zcw_default(dba7a263-0971-452c-b209-d97b9011b4cf): ErrImagePull: Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 06 22:12:27 minikube kubelet[2474]: E0606 22:12:27.197437    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ErrImagePull: \"Error response from daemon: pull access denied for web_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:12:36 minikube kubelet[2474]: E0606 22:12:36.037845    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:12:37 minikube kubelet[2474]: E0606 22:12:37.036092    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_broadcaster:latest\\\"\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:12:42 minikube kubelet[2474]: E0606 22:12:42.036361    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ImagePullBackOff: \"Back-off pulling image \\\"web_service:latest\\\"\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:12:47 minikube kubelet[2474]: E0606 22:12:47.036624    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:12:48 minikube kubelet[2474]: E0606 22:12:48.036507    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_broadcaster:latest\\\"\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:12:56 minikube kubelet[2474]: E0606 22:12:56.036538    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ImagePullBackOff: \"Back-off pulling image \\\"web_service:latest\\\"\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:12:58 minikube kubelet[2474]: E0606 22:12:58.038085    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:13:01 minikube kubelet[2474]: E0606 22:13:01.037525    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_broadcaster:latest\\\"\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"
Jun 06 22:13:09 minikube kubelet[2474]: E0606 22:13:09.034555    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"web-service\" with ImagePullBackOff: \"Back-off pulling image \\\"web_service:latest\\\"\"" pod="default/web-service-deployment-586f94df95-t8zcw" podUID="dba7a263-0971-452c-b209-d97b9011b4cf"
Jun 06 22:13:11 minikube kubelet[2474]: E0606 22:13:11.034783    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-receiver\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_receiver:latest\\\"\"" pod="default/hello-receiver-deployment-5f5d76b494-tjn6v" podUID="99b5d400-5895-4c01-902f-22ce80bba609"
Jun 06 22:13:16 minikube kubelet[2474]: E0606 22:13:16.036617    2474 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-broadcaster\" with ImagePullBackOff: \"Back-off pulling image \\\"hello_broadcaster:latest\\\"\"" pod="default/hello-broadcaster-deployment-6688cf76bb-9crvj" podUID="a9be3daa-af42-4944-8e2a-88733bb8c12d"


==> storage-provisioner [405f54856ccd] <==
I0606 22:02:26.136448       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0606 22:02:26.137921       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [ae813acd9315] <==
I0606 22:02:43.119489       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0606 22:02:43.124052       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0606 22:02:43.124120       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0606 22:03:00.526204       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0606 22:03:00.526330       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a39844a5-b365-45e9-9f8d-d6f65a7c1c0d!
I0606 22:03:00.526349       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"975cdf9d-1fb9-42e5-a35f-fb56982a39f0", APIVersion:"v1", ResourceVersion:"1842", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a39844a5-b365-45e9-9f8d-d6f65a7c1c0d became leader
I0606 22:03:00.627070       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a39844a5-b365-45e9-9f8d-d6f65a7c1c0d!

